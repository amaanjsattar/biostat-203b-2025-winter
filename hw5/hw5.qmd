---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: "Amaan Jogia-Sattar, 206324648"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
  pdf: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(GGally)
library(gtsummary)
library(naniar)
library(lubridate)
library(glmnet)
library(vip)
library(ranger)
library(doParallel)
```

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

1. Data preprocessing and feature engineering.

First, we need to load in the data and preprocess it. We will use the `mimic_icu_cohort_rds` file we created in Homework 4. We do not have to copy mimic_icu_cohort.rds into. Instead, we can use `../hw4/mimiciv_shiny/mimic_icu_cohort.rds`.

```{r}
# Load the data
mimiciv_icu_cohort <- readRDS("../hw4/mimiciv_shiny/mimic_icu_cohort.rds")
```

We can now go ahead with preprocessing. 
Let's take a look at our dataset:
```{r}
head(mimiciv_icu_cohort)
str(mimiciv_icu_cohort)
```

We first adapt our preprocessing code from HW4:
```{r}
mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    first_careunit = fct_lump_n(first_careunit,
                                n = 4,
                                other_level = "Other"),
    last_careunit = fct_lump_n(last_careunit,
                               n = 4,
                               other_level = "Other"),
    admission_type = fct_lump_n(admission_type,
                                n = 4,
                                other_level = "Other"),
    admission_location = fct_lump_n(admission_location,
                                    n = 3,
                                    other_level = "Other"),
    discharge_location = fct_lump_n(discharge_location,
                                    n = 4,
                                    other_level = "Other")
  ) %>%
  # Ensure race is a factor so we can work with its levels
  mutate(race = factor(race)) %>%
  { 
    # Capture the current levels of race
    race_levels <- levels(.$race)
    mutate(., race = fct_collapse(race,
      ASIAN    = race_levels[grep("ASIAN",
                                  race_levels)],
      BLACK    = race_levels[grep("BLACK",
                                  race_levels)],
      HISPANIC = race_levels[grep("HISPANIC",
                                  race_levels)],
      WHITE    = race_levels[grep("WHITE",
                                  race_levels)],
      OTHER    = setdiff(race_levels,
                         c(race_levels[grep("ASIAN",
                                            race_levels)],
                           race_levels[grep("BLACK",
                                            race_levels)],
                           race_levels[grep("HISPANIC",
                                            race_levels)],
                           race_levels[grep("WHITE",
                                            race_levels)]))
    ))
  }

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    insurance = as.factor(insurance),
    language = as.factor(language),
    marital_status = as.factor(marital_status),
    gender = as.factor(gender)
  )

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(los_long = los >= 2) %>%
  mutate(los_long = as.factor(los_long))

mimiciv_icu_cohort <- mimiciv_icu_cohort %>% 
  filter(!is.na(los_long))


mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  select(
    subject_id,
    hadm_id,
    stay_id,
    intime,
    first_careunit,
    los_long,
    admission_type,
    admission_location,
    insurance,
    language,
    marital_status,
    race,
    gender,
    chloride,
    creatinine,
    sodium,
    potassium,
    glucose,
    hematocrit,
    wbc_count,
    bicarbonate,
    `Noninvasive BP Systolic`,
    `Noninvasive BP Diastolic`,
    `Respiratory Rate`,
    `Temperature_F`,
    `Heart Rate`,
    age_intime
  )

mimiciv_icu_cohort
```
Double-checking how our variables are stored:
```{r}
str(mimiciv_icu_cohort)
```
Now, we can check for missing values across our dataset:
```{r}
miss_var_summary(mimiciv_icu_cohort) 
```
We observe that lab results make up the majority of missing values. We will impute numeric variables with the median and categorical variables with the mode.Moreover, we can convert categorical variables into dummy (one-hot) encoded variables. We will also normalize our numeric predictors using centering and scaling. We will use the 'tidymodels' package to do this.
For 'intime', we can extract the hour of admission and represent it cyclically using trigonometric transformations (sine and cosine). This approach is commonly utilized for encoding 24-hour time in machine learning models, and is further explained in https://ianlondon.github.io/posts/encoding-cyclical-features-24-hour-time/. 

Here is our prepared recipe for preprocessing our data.
```{r}
icu_recipe<- recipe(los_long ~ ., data = mimiciv_icu_cohort) %>%
  update_role(subject_id, hadm_id, stay_id, new_role = "ID") %>%
  # Extract the hour from intime
  step_mutate(admission_hour = hour(intime)) %>%
  # Create cyclic features for the hour
  step_mutate(
    hour_sin = sin(2 * pi * admission_hour / 24),
    hour_cos = cos(2 * pi * admission_hour / 24)
  ) %>%
  # Remove the original intime and raw admission_hour if not needed
  step_rm(intime, admission_hour) %>%
  # Remaining steps: imputation, dummy coding, normalization
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

summary(icu_recipe)

```
MODEL 1: LOGISTIC REGRESSION WITH ENET REGULARIZATION

Our first model is as follows:
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```
Now, we can do our initial split of the data.


2. Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.
```{r}
# #| eval: false
set.seed(203)

# sort
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id)

data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )
```

Extracting our training and testing sets:
```{r}
train_data <- training(data_split)
test_data <- testing(data_split)
```

Now, we combine our recipe and logistic regression model into a workflow:
```{r}
logit_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(logit_mod) |>
  print()
```

Now, we can tune our hyperparameters:
```{r}
# Define the tuning grid
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
) |> print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
folds <- vfold_cv(train_data, v = 5, strata = los_long)
```

Having our workflow and tuning grid, we run the grid search: 
```{r}
logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```
We can inspect the results:
```{r}
logit_fit
```
And we can visualize results: 
```{r}
logit_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
This plot will show us how performance (CV AUC) varies with different penalty and mixture settings.

Next, we can review the best-performing models and select the top one:
```{r}
# Show the top 5 models based on ROC AUC
logit_fit |>
  show_best(metric = "roc_auc")

# Select the best model
best_logit <- logit_fit |>
  select_best(metric = "roc_auc")
best_logit
```
We finalize our workflow: 
```{r}
final_logit_wf <- logit_wf |>
  finalize_workflow(best_logit)
final_logit_wf
```

Now, we can fit the final model on the entire trainingg set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_logit_fit <- final_logit_wf |>
  last_fit(data_split)
final_logit_fit

# Collect test metrics
final_logit_fit |> 
  collect_metrics()
```
For our Logistic Regression Model (with ENet Regularization), we observe an AUC of 0.614202 and an accuracy of 0.5820469. 

Examining feature importance: 
```{r}
final_logit_model <- final_logit_fit %>% 
  extract_fit_parsnip()

# Create a VIP plot:
vip(final_logit_model, num_features = 20)
```
It appears that the cosine component of hour of admission `hour_cos`, indicator for admission location `admission_location_TRANSFER.FROM_HOSPITAL`, `Heart Rate`, indicator for first care unit `first_careunit_Medical.Surgical.Intensive.Care.Unit..MICU.SICU.`, `Respiratory Rate`, and `Noninvasive BP Systolic` were among the most important features.

MODEL 2: RANDOM FOREST
We began this process using a coarser tuning grid to identify a promising region of the parameter space. We then refined our grid search within this region. We utilized parallel processing to expedite the tuning process.
First, we define our random forest model:
```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),    # number of predictors randomly sampled at each split
    trees = tune()    # number of trees in the ensemble
  ) %>% 
  set_engine("ranger", importance = "impurity")

rf_mod
```
Next, we combine our recipe and random forest model into a workflow:
```{r}
rf_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(rf_mod) |>
  print()
```

Next, we define our tuning grid:
```{r}
param_grid_rf <- grid_regular(
  trees(range = c(100L, 250L)),
  mtry(range = c(1L, 3L)),
  levels = c(3, 3)
) %>% print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
rf_folds <- vfold_cv(train_data, v = 5, strata = los_long)
rf_folds
```
Set up parallel processing: use all cores minus one.
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})

```


Having our workflow and tuning grid, we run the grid search:
```{r}
rf_fit_coarse <- rf_wf |>
  tune_grid(
    resamples = rf_folds,
    grid = param_grid_rf,
    metrics = metric_set(roc_auc, accuracy)
  )
rf_fit_coarse
```
Stop the parallel cluster after tuning:
```{r}
stopCluster(cl)
```


Visualizing the results: 
```{r}
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = factor(mtry))) +
  geom_point() +
  labs(x = "Number of Trees", y = "CV AUC")
```

Next, we can review the best-performing models and select the top one:
```{r}
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

best_rf <- rf_fit_coarse %>%
  select_best(metric = "roc_auc")
best_rf
```

We finalize our workflow:
```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_rf_wf
```

Now, we can fit the final model on the entire training set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_rf_fit <- final_rf_wf %>%
  last_fit(data_split)
final_rf_fit
```

Collect test metrics:
```{r}
final_rf_fit %>%
  collect_metrics()
```

```{r}
# Show all metrics for each combination in your grid
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf)

# Display the top 5 models based on ROC AUC
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

```
Now that we have run the coarse grid search, we can refine our grid search within the promising region of the parameter space. We will use the best-performing model from the coarse grid search as a starting point.
```{r}
# Select the best parameters based on ROC AUC from your coarse grid tuning
best_rf <- rf_fit_coarse %>% 
  select_best(metric = "roc_auc")
print(best_rf)

# Finalize your workflow using the best hyperparameters (mtry = 3 and trees = 250)
final_rf_wf <- rf_wf %>% 
  finalize_workflow(best_rf)
print(final_rf_wf)

# Fit the final random forest model on the entire training set and evaluate on the test set
final_rf_fit <- final_rf_wf %>% 
  last_fit(data_split)
print(final_rf_fit)

# Collect and display test set metrics (ROC AUC and Accuracy)
final_rf_metrics <- final_rf_fit %>% 
  collect_metrics()
print(final_rf_metrics)

# Optionally, extract the fitted model and generate a variable importance plot
final_rf_model <- final_rf_fit %>% extract_fit_parsnip()
vip(final_rf_model, num_features = 20)
```

For our Random Forest Model, we observe an AUC of 0.64527210 and an accuracy of 0.6017195. Examining feature importance based on 'Impurity', we observe that there is a group of features with relatively large importance values, followed by a drop-off in the variable importance plot. Namely, we observe that `Noninvasive BP Systolic`, `wbc_count`, `Heart Rate`, `Temperature_F`, `hematocrit`, `Noninvasive BP Diastolic`, `Respiratory Rate`, `age_intime`, `glucose`, `hour_cos`, `creatinine`, `potassium`, `chloride`, `bicarbonate`, `sodium`, and the sin component of hour of admission `hour_sin` were among the most important features. Interestingly, it appears that clinical vital signs and lab results were among the most important features in the Random Forest Model. This is a departure from what we observed in terms of variable importance in the Logistic Regression Model, where some of the categorical features recorded at the time of admission were also in the upper section of the variable importance plot.


4. Compare model classification performance on the test set. Report both the area under ROC curve and accuracy for each machine learning algorithm and the model stacking. Interpret the results. What are the most important features in predicting long ICU stays? How do the models compare in terms of performance and interpretability?
