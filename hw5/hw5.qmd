---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: "Amaan Jogia-Sattar, 206324648"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
  pdf: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(GGally)
library(gtsummary)
library(naniar)
library(lubridate)
library(glmnet)
library(vip)
library(ranger)
library(doParallel)
library(xgboost)
```

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

First, we need to load in the data and preprocess it. We will use the `mimic_icu_cohort_rds` file we created in Homework 4. We do not have to copy mimic_icu_cohort.rds into. Instead, we can use `../hw4/mimiciv_shiny/mimic_icu_cohort.rds`.

```{r}
# Load the data
mimiciv_icu_cohort <- readRDS("../hw4/mimiciv_shiny/mimic_icu_cohort.rds")
```

We can now go ahead with preprocessing. 
Let's take a look at our dataset:
```{r}
head(mimiciv_icu_cohort)
str(mimiciv_icu_cohort)
```

We first adapt our preprocessing code from HW4:
```{r}
mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    first_careunit = fct_lump_n(first_careunit,
                                n = 4,
                                other_level = "Other"),
    last_careunit = fct_lump_n(last_careunit,
                               n = 4,
                               other_level = "Other"),
    admission_type = fct_lump_n(admission_type,
                                n = 4,
                                other_level = "Other"),
    admission_location = fct_lump_n(admission_location,
                                    n = 3,
                                    other_level = "Other"),
    discharge_location = fct_lump_n(discharge_location,
                                    n = 4,
                                    other_level = "Other")
  ) %>%
  # Ensure race is a factor so we can work with its levels
  mutate(race = factor(race)) %>%
  { 
    # Capture the current levels of race
    race_levels <- levels(.$race)
    mutate(., race = fct_collapse(race,
      ASIAN    = race_levels[grep("ASIAN",
                                  race_levels)],
      BLACK    = race_levels[grep("BLACK",
                                  race_levels)],
      HISPANIC = race_levels[grep("HISPANIC",
                                  race_levels)],
      WHITE    = race_levels[grep("WHITE",
                                  race_levels)],
      OTHER    = setdiff(race_levels,
                         c(race_levels[grep("ASIAN",
                                            race_levels)],
                           race_levels[grep("BLACK",
                                            race_levels)],
                           race_levels[grep("HISPANIC",
                                            race_levels)],
                           race_levels[grep("WHITE",
                                            race_levels)]))
    ))
  }

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    insurance = as.factor(insurance),
    language = as.factor(language),
    marital_status = as.factor(marital_status),
    gender = as.factor(gender)
  )

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(los_long = los >= 2) %>%
  mutate(los_long = as.factor(los_long))

mimiciv_icu_cohort <- mimiciv_icu_cohort %>% 
  filter(!is.na(los_long))


mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  select(
    subject_id,
    hadm_id,
    stay_id,
    intime,
    first_careunit,
    los_long,
    admission_type,
    admission_location,
    insurance,
    language,
    marital_status,
    race,
    gender,
    chloride,
    creatinine,
    sodium,
    potassium,
    glucose,
    hematocrit,
    wbc_count,
    bicarbonate,
    `Noninvasive BP Systolic`,
    `Noninvasive BP Diastolic`,
    `Respiratory Rate`,
    `Temperature_F`,
    `Heart Rate`,
    age_intime
  )

mimiciv_icu_cohort
```
Double-checking how our variables are stored:
```{r}
str(mimiciv_icu_cohort)
```
Now, we can check for missing values across our dataset:
```{r}
miss_var_summary(mimiciv_icu_cohort) 
```
We observe that lab results make up the majority of missing values. We will impute numeric variables with the median and categorical variables with the mode.Moreover, we can convert categorical variables into dummy (one-hot) encoded variables. We will also normalize our numeric predictors using centering and scaling. We will use the 'tidymodels' package to do this.
For 'intime', we can extract the hour of admission and represent it cyclically using trigonometric transformations (sine and cosine). This approach is commonly utilized for encoding 24-hour time in machine learning models, and is further explained in https://ianlondon.github.io/posts/encoding-cyclical-features-24-hour-time/. 

Here is our prepared recipe for preprocessing our data.
```{r}
icu_recipe<- recipe(los_long ~ ., data = mimiciv_icu_cohort) %>%
  update_role(subject_id, hadm_id, stay_id, new_role = "ID") %>%
  # Extract the hour from intime
  step_mutate(admission_hour = hour(intime)) %>%
  # Create cyclic features for the hour
  step_mutate(
    hour_sin = sin(2 * pi * admission_hour / 24),
    hour_cos = cos(2 * pi * admission_hour / 24)
  ) %>%
  # Remove the original intime and raw admission_hour if not needed
  step_rm(intime, admission_hour) %>%
  # Remaining steps: imputation, dummy coding, normalization
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

summary(icu_recipe)

```
MODEL 1: LOGISTIC REGRESSION WITH ENET REGULARIZATION

Our first model is as follows:
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```
Now, we can do our initial split of the data.


Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.
```{r}
# #| eval: false
set.seed(203)

# sort
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id)

data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )
```

Extracting our training and testing sets:
```{r}
train_data <- training(data_split)
test_data <- testing(data_split)
```

Now, we combine our recipe and logistic regression model into a workflow:
```{r}
logit_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(logit_mod) |>
  print()
```

Now, we can tune our hyperparameters:
```{r}
# Define the tuning grid
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
) |> print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
folds <- vfold_cv(train_data, v = 5, strata = los_long)
```

Having our workflow and tuning grid, we run the grid search: 
```{r}
logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```
We can inspect the results:
```{r}
logit_fit
```
And we can visualize results: 
```{r}
logit_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
This plot will show us how performance (CV AUC) varies with different penalty and mixture settings.

Next, we can review the best-performing models and select the top one:
```{r}
# Show the top 5 models based on ROC AUC
logit_fit |>
  show_best(metric = "roc_auc")

# Select the best model
best_logit <- logit_fit |>
  select_best(metric = "roc_auc")
best_logit
```
We finalize our workflow: 
```{r}
final_logit_wf <- logit_wf |>
  finalize_workflow(best_logit)
final_logit_wf
```

Now, we can fit the final model on the entire trainingg set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_logit_fit <- final_logit_wf |>
  last_fit(data_split)
final_logit_fit

# Collect test metrics
final_logit_fit |> 
  collect_metrics()
```
For our Logistic Regression Model (with ENet Regularization), we observe an AUC of 0.614202 and an accuracy of 0.5820469. 

Examining feature importance: 
```{r}
final_logit_model <- final_logit_fit %>% 
  extract_fit_parsnip()

# Create a VIP plot:
vip(final_logit_model, num_features = 20)
```
It appears that the cosine component of hour of admission `hour_cos`, indicator for admission location `admission_location_TRANSFER.FROM_HOSPITAL`, `Heart Rate`, indicator for first care unit `first_careunit_Medical.Surgical.Intensive.Care.Unit..MICU.SICU.`, `Respiratory Rate`, and `Noninvasive BP Systolic` were among the most important features.

MODEL 2: RANDOM FOREST
We began this process using a coarser tuning grid to identify a promising region of the parameter space. We then refined our grid search within this region. We utilized parallel processing to expedite the tuning process.
First, we define our random forest model:
```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),    # number of predictors randomly sampled at each split
    trees = tune()    # number of trees in the ensemble
  ) %>% 
  set_engine("ranger", importance = "impurity")

rf_mod
```
Next, we combine our recipe and random forest model into a workflow:
```{r}
rf_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(rf_mod) |>
  print()
```

Next, we define our tuning grid:
```{r}
param_grid_rf <- grid_regular(
  trees(range = c(100L, 250L)),
  mtry(range = c(1L, 3L)),
  levels = c(3, 3)
) %>% print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
rf_folds <- vfold_cv(train_data, v = 5, strata = los_long)
rf_folds
```
Set up parallel processing: use all cores minus one.
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})

```


Having our workflow and tuning grid, we run the grid search:
```{r}
rf_fit_coarse <- rf_wf |>
  tune_grid(
    resamples = rf_folds,
    grid = param_grid_rf,
    metrics = metric_set(roc_auc, accuracy)
  )
rf_fit_coarse
```
Stop the parallel cluster after tuning:
```{r}
stopCluster(cl)
```


Visualizing the results: 
```{r}
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = factor(mtry))) +
  geom_point() +
  labs(x = "Number of Trees", y = "CV AUC")
```

Next, we can review the best-performing models and select the top one:
```{r}
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

best_rf <- rf_fit_coarse %>%
  select_best(metric = "roc_auc")
best_rf
```

We finalize our workflow:
```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_rf_wf
```

Now, we can fit the final model on the entire training set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_rf_fit <- final_rf_wf %>%
  last_fit(data_split)
final_rf_fit
```

Collect test metrics:
```{r}
final_rf_fit %>%
  collect_metrics()
```

```{r}
# Show all metrics for each combination in your grid
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf)

# Display the top 5 models based on ROC AUC
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

```
Now that we have run the coarse grid search, we can refine our grid search within the promising region of the parameter space. We will use the best-performing model from the coarse grid search as a starting point.
```{r}
# Select the best parameters based on ROC AUC from your coarse grid tuning
best_rf <- rf_fit_coarse %>% 
  select_best(metric = "roc_auc")
print(best_rf)

# Finalize your workflow using the best hyperparameters (mtry = 3 and trees = 250)
final_rf_wf <- rf_wf %>% 
  finalize_workflow(best_rf)
print(final_rf_wf)

# Fit the final random forest model on the entire training set and evaluate on the test set
final_rf_fit <- final_rf_wf %>% 
  last_fit(data_split)
print(final_rf_fit)

# Collect and display test set metrics (ROC AUC and Accuracy)
final_rf_metrics <- final_rf_fit %>% 
  collect_metrics()
print(final_rf_metrics)

# Optionally, extract the fitted model and generate a variable importance plot
final_rf_model <- final_rf_fit %>% extract_fit_parsnip()
vip(final_rf_model, num_features = 20)
```

For our Random Forest Model, we observe an AUC of 0.64527210 and an accuracy of 0.6017195. Examining feature importance based on 'Impurity', we observe that there is a group of features with relatively large importance values, followed by a drop-off in the variable importance plot. Namely, we observe that `Noninvasive BP Systolic`, `wbc_count`, `Heart Rate`, `Temperature_F`, `hematocrit`, `Noninvasive BP Diastolic`, `Respiratory Rate`, `age_intime`, `glucose`, `hour_cos`, `creatinine`, `potassium`, `chloride`, `bicarbonate`, `sodium`, and the sin component of hour of admission `hour_sin` were among the most important features. Interestingly, it appears that clinical vital signs and lab results were among the most important features in the Random Forest Model. This is a departure from what we observed in terms of variable importance in the Logistic Regression Model, where some of the categorical features recorded at the time of admission were also in the upper section of the variable importance plot.

MODEL 3: BOOSTING
We proceed to fit a boosting model. First, we will define our boosting model with tunable hyperparameters:
```{r}
# Define the XGBoost model with tunable parameters
xgb_mod <- boost_tree(
  mode = "classification",
  trees      = tune(),   # total number of trees
  tree_depth = tune(),   # maximum tree depth
  learn_rate = tune()    # learning rate
) %>% 
  set_engine("xgboost")
xgb_mod
```
Next, we combine our recipe and boosting model into a workflow:
```{r}
xgb_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(xgb_mod) |>
  print()
```
Next, we define our tuning grid:
```{r}
param_grid_xgb <- grid_regular(
  trees(range = c(100L, 500L)),
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 3, 3)) %>% 
  print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
xgb_folds <- vfold_cv(train_data, v = 5, strata = los_long)
xgb_folds
```
Next, we tune our model using the grid search. We will use parallel processing to expedite the tuning process.
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})
```

```{r}
xgb_fit <- xgb_wf |>
  tune_grid(
    resamples = xgb_folds,
    grid = param_grid_xgb,
    metrics = metric_set(roc_auc, accuracy)
  )
xgb_fit
```
Now, we can terminate our parallel processing:
```{r}
stopCluster(cl)
```

Visualizing the results:
```{r}
xgb_fit %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

Next, we can review the best-performing models and select the top one:
```{r}
xgb_fit %>%
  show_best(metric = "roc_auc")
```
Next, we select the best tuning parameters based on ROC AUC:
```{r}
best_xgb <- xgb_fit %>%
  select_best(metric = "roc_auc")
best_xgb
```

We finalize our workflow:
```{r}
final_xgb_wf <- xgb_wf %>%
  finalize_workflow(best_xgb)
final_xgb_wf
```

Now, we can fit the final model on the entire training set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_xgb_fit <- final_xgb_wf %>%
  last_fit(data_split)
final_xgb_fit
```

Collect test metrics:
```{r}
final_xgb_fit %>%
  collect_metrics() %>%
  print()
```

Examining feature importance:
```{r}
final_xgb_model <- final_xgb_fit %>%
  extract_fit_parsnip()

vip(final_xgb_model, num_features = 20)
```


For our Boosting Model, we observe an AUC of 0.6480967 and an accuracy of 0.6053618. Examining feature importance, we observe that there isn't necessarily a sharp 'drop-off' in the variable importance plot, so we will identify the top 10 features based on importance. Namely, we observe that `Temperature_F`, the hour component of admission time `hour_cos`, `Noninvasive BP Systolic`, the indicator for admission location `admission_location_TRANSFER.FROM_HOSPITAL`, `age_intime`, `wbc_count`, `Respiratory Rate`, `creatinine`, `Heart Rate`, and `hematocrit` were among the most important features.

We have now created our three non-stacked models, and we can summarize their performance and feature importance. We can extract the top ten features from each model:
```{r}
# For Logistic Regression:
final_logit_model <- final_logit_fit %>% extract_fit_parsnip()
top10_logit <- vi(final_logit_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("Top 10 features for Logistic Regression Model:\n")
print(top10_logit)

# For Random Forest:
final_rf_model <- final_rf_fit %>% extract_fit_parsnip()
top10_rf <- vi(final_rf_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("\nTop 10 features for Random Forest Model:\n")
print(top10_rf)

# For XGBoost:
final_xgb_model <- final_xgb_fit %>% extract_fit_parsnip()
top10_xgb <- vi(final_xgb_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("\nTop 10 features for XGBoost Model:\n")
print(top10_xgb)
```
It would be interesting to see if there are any features that were commonly identified as important across all three models:
```{r}
# Compute the intersection of the top 10 variable names from each model
common_features <- Reduce(intersect, list(top10_logit$Variable,
                                            top10_rf$Variable,
                                            top10_xgb$Variable))
cat("Common features across all top 10 lists:\n")
print(common_features)

```

We observe that across our Logistic Regression Model with Elastic Net Regularization, our Random Forest Model, and our Boosting Model, these features consistently appeared in the top ten in importance: `hour_cos`, `Heart Rate`, `Respiratory Rate`, `Noninvasive BP Systolic`, `wbc_count`, and `hematocrit`. 

Now, we can chronicle the observed performance on the test set from each of these models, in terms of AUC and Accuracy:
```{r}
logit_metrics <- final_logit_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "Logistic Regression")

rf_metrics <- final_rf_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "Random Forest")

xgb_metrics <- final_xgb_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "XGBoost")

# Combine the metrics and filter for roc_auc and accuracy, using .estimate as the metric value
performance_metrics <- bind_rows(logit_metrics, rf_metrics, xgb_metrics) %>%
  filter(.metric %in% c("roc_auc", "accuracy")) %>%
  select(Model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

performance_metrics
```
We observe that the Random Forest and XGBoost models had similar overall performance on the test set, while logistic regression performed slightly worse in terms of these two performance metrics. There is certainly a tradeoff between model accuracy and interpretability to be explored, and the decision to report one model over another should be made with careful consideration of intention and audience. For instance, a statistician prioritizing interpretability such that they can communicate generalizable results to clinician colleagues may opt for logistic regression in this instance, despite its relatively lower performance. Meanwhile, a machine learning engineer hoping to develop a prediction algorithm for length of ICU stay based on clinical features may opt to maximize their performance at the expense of interpretability, while still maintaining generalizable results. An individual participating in a hackathon or statistical modeling competition who simply aims to achieve maximal performance on the unseen test data may fully prioritize performance at the expense of gleaning insights from the model, possibly resorting to more "black-box" complex algorithmic approaches that leverage high computational power. Ultimately, our models all predicted length of ICU stay within a similar "neighborhood" of accuracy, and it will be interesting to compare their individual performance with that of a stacked model.


