---
title: "Biostat 203B Homework 5"
subtitle: Due Mar 20 @ 11:59PM
author: "Amaan Jogia-Sattar, 206324648"
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
  pdf: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
library(tidymodels)
library(GGally)
library(gtsummary)
library(naniar)
library(lubridate)
library(glmnet)
library(vip)
library(ranger)
library(doParallel)
library(xgboost)
library(stacks)
library(yardstick)
library(purrr)
```

## Predicting ICU duration

Using the ICU cohort `mimiciv_icu_cohort.rds` you built in Homework 4, develop at least three machine learning approaches (logistic regression with enet regularization, random forest, boosting, SVM, MLP, etc) plus a model stacking approach for predicting whether a patient's ICU stay will be longer than 2 days. You should use the `los_long` variable as the outcome. You algorithms can use patient demographic information (gender, age at ICU `intime`, marital status, race), ICU admission information (first care unit), the last lab measurements before the ICU stay, and first vital measurements during ICU stay as features. You are welcome to use any feature engineering techniques you think are appropriate; but make sure to not use features that are not available at an ICU stay's `intime`. For instance, `last_careunit` cannot be used in your algorithms. 

First, we need to load in the data and preprocess it. We will use the `mimic_icu_cohort_rds` file we created in Homework 4. We do not have to copy mimic_icu_cohort.rds into. Instead, we can use `../hw4/mimiciv_shiny/mimic_icu_cohort.rds`.

```{r}
# Load the data
mimiciv_icu_cohort <- readRDS("../hw4/mimiciv_shiny/mimic_icu_cohort.rds")
```

We can now go ahead with preprocessing. 
Let's take a look at our dataset:
```{r}
head(mimiciv_icu_cohort)
str(mimiciv_icu_cohort)
```

We first adapt our preprocessing code from HW4:
```{r}
mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    first_careunit = fct_lump_n(first_careunit,
                                n = 4,
                                other_level = "Other"),
    last_careunit = fct_lump_n(last_careunit,
                               n = 4,
                               other_level = "Other"),
    admission_type = fct_lump_n(admission_type,
                                n = 4,
                                other_level = "Other"),
    admission_location = fct_lump_n(admission_location,
                                    n = 3,
                                    other_level = "Other"),
    discharge_location = fct_lump_n(discharge_location,
                                    n = 4,
                                    other_level = "Other")
  ) %>%
  # Ensure race is a factor so we can work with its levels
  mutate(race = factor(race)) %>%
  { 
    # Capture the current levels of race
    race_levels <- levels(.$race)
    mutate(., race = fct_collapse(race,
      ASIAN    = race_levels[grep("ASIAN",
                                  race_levels)],
      BLACK    = race_levels[grep("BLACK",
                                  race_levels)],
      HISPANIC = race_levels[grep("HISPANIC",
                                  race_levels)],
      WHITE    = race_levels[grep("WHITE",
                                  race_levels)],
      OTHER    = setdiff(race_levels,
                         c(race_levels[grep("ASIAN",
                                            race_levels)],
                           race_levels[grep("BLACK",
                                            race_levels)],
                           race_levels[grep("HISPANIC",
                                            race_levels)],
                           race_levels[grep("WHITE",
                                            race_levels)]))
    ))
  }

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(
    insurance = as.factor(insurance),
    language = as.factor(language),
    marital_status = as.factor(marital_status),
    gender = as.factor(gender)
  )

mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  mutate(los_long = los >= 2) %>%
  mutate(los_long = as.factor(los_long))

mimiciv_icu_cohort <- mimiciv_icu_cohort %>% 
  filter(!is.na(los_long))


mimiciv_icu_cohort <- mimiciv_icu_cohort %>%
  select(
    subject_id,
    hadm_id,
    stay_id,
    intime,
    first_careunit,
    los_long,
    admission_type,
    admission_location,
    insurance,
    language,
    marital_status,
    race,
    gender,
    chloride,
    creatinine,
    sodium,
    potassium,
    glucose,
    hematocrit,
    wbc_count,
    bicarbonate,
    `Noninvasive BP Systolic`,
    `Noninvasive BP Diastolic`,
    `Respiratory Rate`,
    `Temperature_F`,
    `Heart Rate`,
    age_intime
  )

mimiciv_icu_cohort
```
Double-checking how our variables are stored:
```{r}
str(mimiciv_icu_cohort)
```
Now, we can check for missing values across our dataset:
```{r}
miss_var_summary(mimiciv_icu_cohort) 
```
We observe that lab results make up the majority of missing values. We will impute numeric variables with the median and categorical variables with the mode.Moreover, we can convert categorical variables into dummy (one-hot) encoded variables. We will also normalize our numeric predictors using centering and scaling. We will use the 'tidymodels' package to do this.
For 'intime', we can extract the hour of admission and represent it cyclically using trigonometric transformations (sine and cosine). This approach is commonly utilized for encoding 24-hour time in machine learning models, and is further explained in https://ianlondon.github.io/posts/encoding-cyclical-features-24-hour-time/. 

Here is our prepared recipe for preprocessing our data.
```{r}
icu_recipe<- recipe(los_long ~ ., data = mimiciv_icu_cohort) %>%
  update_role(subject_id, hadm_id, stay_id, new_role = "ID") %>%
  # Extract the hour from intime
  step_mutate(admission_hour = hour(intime)) %>%
  # Create cyclic features for the hour
  step_mutate(
    hour_sin = sin(2 * pi * admission_hour / 24),
    hour_cos = cos(2 * pi * admission_hour / 24)
  ) %>%
  # Remove the original intime and raw admission_hour if not needed
  step_rm(intime, admission_hour) %>%
  # Remaining steps: imputation, dummy coding, normalization
  step_impute_median(all_numeric_predictors()) %>%
  step_impute_mode(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_numeric_predictors())

summary(icu_recipe)

```
MODEL 1: LOGISTIC REGRESSION WITH ENET REGULARIZATION

Our first model is as follows:
```{r}
logit_mod <- 
  logistic_reg(
    penalty = tune(), 
    mixture = tune()
  ) |> 
  set_engine("glmnet", standardize = FALSE) |>
  print()
```
Now, we can do our initial split of the data.


Partition data into 50% training set and 50% test set. Stratify partitioning according to `los_long`. For grading purpose, sort the data by `subject_id`, `hadm_id`, and `stay_id` and use the seed `203` for the initial data split. Below is the sample code.
```{r}
# #| eval: false
set.seed(203)

# sort
mimiciv_icu_cohort <- mimiciv_icu_cohort |>
  arrange(subject_id, hadm_id, stay_id)

data_split <- initial_split(
  mimiciv_icu_cohort, 
  # stratify by los_long
  strata = "los_long", 
  prop = 0.5
  )
```

Extracting our training and testing sets:
```{r}
train_data <- training(data_split)
test_data <- testing(data_split)
```

Now, we combine our recipe and logistic regression model into a workflow:
```{r}
logit_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(logit_mod) |>
  print()
```

Now, we can tune our hyperparameters:
```{r}
# Define the tuning grid
param_grid <- grid_regular(
  penalty(range = c(-6, 3)), 
  mixture(),
  levels = c(100, 5)
) |> print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
folds <- vfold_cv(train_data, v = 5, strata = los_long)
```

Having our workflow and tuning grid, we run the grid search: 
```{r}
logit_fit <- logit_wf |>
  tune_grid(
    resamples = folds,
    grid = param_grid,
    metrics = metric_set(roc_auc, accuracy)
  )
```
We can inspect the results:
```{r}
logit_fit
```
And we can visualize results: 
```{r}
logit_fit |>
  collect_metrics() |>
  print(width = Inf) |>
  filter(.metric == "roc_auc") |>
  ggplot(mapping = aes(x = penalty, y = mean, color = factor(mixture))) +
  geom_point() +
  labs(x = "Penalty", y = "CV AUC") +
  scale_x_log10()
```
This plot will show us how performance (CV AUC) varies with different penalty and mixture settings.

Next, we can review the best-performing models and select the top one:
```{r}
# Show the top 5 models based on ROC AUC
logit_fit |>
  show_best(metric = "roc_auc")

# Select the best model
best_logit <- logit_fit |>
  select_best(metric = "roc_auc")
best_logit
```
We finalize our workflow: 
```{r}
final_logit_wf <- logit_wf |>
  finalize_workflow(best_logit)
final_logit_wf
```

Now, we can fit the final model on the entire trainingg set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_logit_fit <- final_logit_wf |>
  last_fit(data_split)
final_logit_fit

# Collect test metrics
final_logit_fit |> 
  collect_metrics()
```
For our Logistic Regression Model (with ENet Regularization), we observe an AUC of 0.614202 and an accuracy of 0.5820469. 

Examining feature importance: 
```{r}
final_logit_model <- final_logit_fit %>% 
  extract_fit_parsnip()

# Create a VIP plot:
vip(final_logit_model, num_features = 20)
```
It appears that the cosine component of hour of admission `hour_cos`, indicator for admission location `admission_location_TRANSFER.FROM_HOSPITAL`, `Heart Rate`, indicator for first care unit `first_careunit_Medical.Surgical.Intensive.Care.Unit..MICU.SICU.`, `Respiratory Rate`, and `Noninvasive BP Systolic` were among the most important features.

MODEL 2: RANDOM FOREST
We began this process using a coarser tuning grid to identify a promising region of the parameter space. We then refined our grid search within this region. We utilized parallel processing to expedite the tuning process.
First, we define our random forest model:
```{r}
rf_mod <- 
  rand_forest(
    mode = "classification",
    mtry = tune(),    # number of predictors randomly sampled at each split
    trees = tune()    # number of trees in the ensemble
  ) %>% 
  set_engine("ranger", importance = "impurity")

rf_mod
```
Next, we combine our recipe and random forest model into a workflow:
```{r}
rf_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(rf_mod) |>
  print()
```

Next, we define our tuning grid:
```{r}
param_grid_rf <- grid_regular(
  trees(range = c(100L, 250L)),
  mtry(range = c(1L, 3L)),
  levels = c(3, 3)
) %>% print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
rf_folds <- vfold_cv(train_data, v = 5, strata = los_long)
rf_folds
```
Set up parallel processing: use all cores minus one.
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})

```


Having our workflow and tuning grid, we run the grid search:
```{r}
rf_fit_coarse <- rf_wf |>
  tune_grid(
    resamples = rf_folds,
    grid = param_grid_rf,
    metrics = metric_set(roc_auc, accuracy)
  )
rf_fit_coarse
```
Stop the parallel cluster after tuning:
```{r}
stopCluster(cl)
```


Visualizing the results: 
```{r}
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf) %>%
  filter(.metric == "roc_auc") %>%
  ggplot(mapping = aes(x = trees, y = mean, color = factor(mtry))) +
  geom_point() +
  labs(x = "Number of Trees", y = "CV AUC")
```

Next, we can review the best-performing models and select the top one:
```{r}
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

best_rf <- rf_fit_coarse %>%
  select_best(metric = "roc_auc")
best_rf
```

We finalize our workflow:
```{r}
final_rf_wf <- rf_wf %>%
  finalize_workflow(best_rf)
final_rf_wf
```

Now, we can fit the final model on the entire training set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_rf_fit <- final_rf_wf %>%
  last_fit(data_split)
final_rf_fit
```

Collect test metrics:
```{r}
final_rf_fit %>%
  collect_metrics()
```

```{r}
# Show all metrics for each combination in your grid
rf_fit_coarse %>%
  collect_metrics() %>%
  print(width = Inf)

# Display the top 5 models based on ROC AUC
rf_fit_coarse %>%
  show_best(metric = "roc_auc")

```
Now that we have run the coarse grid search, we can refine our grid search within the promising region of the parameter space. We will use the best-performing model from the coarse grid search as a starting point.
```{r}
# Select the best parameters based on ROC AUC from your coarse grid tuning
best_rf <- rf_fit_coarse %>% 
  select_best(metric = "roc_auc")
print(best_rf)

# Finalize your workflow using the best hyperparameters (mtry = 3 and trees = 250)
final_rf_wf <- rf_wf %>% 
  finalize_workflow(best_rf)
print(final_rf_wf)

# Fit the final random forest model on the entire training set and evaluate on the test set
final_rf_fit <- final_rf_wf %>% 
  last_fit(data_split)
print(final_rf_fit)

# Collect and display test set metrics (ROC AUC and Accuracy)
final_rf_metrics <- final_rf_fit %>% 
  collect_metrics()
print(final_rf_metrics)

# Optionally, extract the fitted model and generate a variable importance plot
final_rf_model <- final_rf_fit %>% extract_fit_parsnip()
vip(final_rf_model, num_features = 20)
```

For our Random Forest Model, we observe an AUC of 0.64527210 and an accuracy of 0.6017195. Examining feature importance based on 'Impurity', we observe that there is a group of features with relatively large importance values, followed by a drop-off in the variable importance plot. Namely, we observe that `Noninvasive BP Systolic`, `wbc_count`, `Heart Rate`, `Temperature_F`, `hematocrit`, `Noninvasive BP Diastolic`, `Respiratory Rate`, `age_intime`, `glucose`, `hour_cos`, `creatinine`, `potassium`, `chloride`, `bicarbonate`, `sodium`, and the sin component of hour of admission `hour_sin` were among the most important features. Interestingly, it appears that clinical vital signs and lab results were among the most important features in the Random Forest Model. This is a departure from what we observed in terms of variable importance in the Logistic Regression Model, where some of the categorical features recorded at the time of admission were also in the upper section of the variable importance plot.

MODEL 3: BOOSTING
We proceed to fit a boosting model. First, we will define our boosting model with tunable hyperparameters:
```{r}
# Define the XGBoost model with tunable parameters
xgb_mod <- boost_tree(
  mode = "classification",
  trees      = tune(),   # total number of trees
  tree_depth = tune(),   # maximum tree depth
  learn_rate = tune()    # learning rate
) %>% 
  set_engine("xgboost")
xgb_mod
```
Next, we combine our recipe and boosting model into a workflow:
```{r}
xgb_wf <- workflow() |>
  add_recipe(icu_recipe) |>
  add_model(xgb_mod) |>
  print()
```
Next, we define our tuning grid:
```{r}
param_grid_xgb <- grid_regular(
  trees(range = c(100L, 500L)),
  tree_depth(range = c(1L, 3L)),
  learn_rate(range = c(-5, 2), trans = log10_trans()),
  levels = c(3, 3, 3)) %>% 
  print()
```

Next, we set cross-validation partitioning, creating 5 folds:
```{r}
set.seed(203)
xgb_folds <- vfold_cv(train_data, v = 5, strata = los_long)
xgb_folds
```
Next, we tune our model using the grid search. We will use parallel processing to expedite the tuning process.
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})
```

```{r}
xgb_fit <- xgb_wf |>
  tune_grid(
    resamples = xgb_folds,
    grid = param_grid_xgb,
    metrics = metric_set(roc_auc, accuracy)
  )
xgb_fit
```
Now, we can terminate our parallel processing:
```{r}
stopCluster(cl)
```

Visualizing the results:
```{r}
xgb_fit %>%
  collect_metrics() %>%
  filter(.metric == "roc_auc") %>%
  ggplot(aes(x = learn_rate, y = mean, color = factor(tree_depth))) +
  geom_point() +
  labs(x = "Learning Rate", y = "CV AUC") +
  scale_x_log10()
```

Next, we can review the best-performing models and select the top one:
```{r}
xgb_fit %>%
  show_best(metric = "roc_auc")
```
Next, we select the best tuning parameters based on ROC AUC:
```{r}
best_xgb <- xgb_fit %>%
  select_best(metric = "roc_auc")
best_xgb
```

We finalize our workflow:
```{r}
final_xgb_wf <- xgb_wf %>%
  finalize_workflow(best_xgb)
final_xgb_wf
```

Now, we can fit the final model on the entire training set and evaluate it on the test set using the `last_fit()` function:
```{r}
final_xgb_fit <- final_xgb_wf %>%
  last_fit(data_split)
final_xgb_fit
```

Collect test metrics:
```{r}
final_xgb_fit %>%
  collect_metrics() %>%
  print()
```

Examining feature importance:
```{r}
final_xgb_model <- final_xgb_fit %>%
  extract_fit_parsnip()

vip(final_xgb_model, num_features = 20)
```


For our Boosting Model, we observe an AUC of 0.6480967 and an accuracy of 0.6053618. Examining feature importance, we observe that there isn't necessarily a sharp 'drop-off' in the variable importance plot, so we will identify the top 10 features based on importance. Namely, we observe that `Temperature_F`, the hour component of admission time `hour_cos`, `Noninvasive BP Systolic`, the indicator for admission location `admission_location_TRANSFER.FROM_HOSPITAL`, `age_intime`, `wbc_count`, `Respiratory Rate`, `creatinine`, `Heart Rate`, and `hematocrit` were among the most important features.

We have now created our three non-stacked models, and we can summarize their performance and feature importance. We can extract the top ten features from each model:
```{r}
# For Logistic Regression:
final_logit_model <- final_logit_fit %>% extract_fit_parsnip()
top10_logit <- vi(final_logit_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("Top 10 features for Logistic Regression Model:\n")
print(top10_logit)

# For Random Forest:
final_rf_model <- final_rf_fit %>% extract_fit_parsnip()
top10_rf <- vi(final_rf_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("\nTop 10 features for Random Forest Model:\n")
print(top10_rf)

# For XGBoost:
final_xgb_model <- final_xgb_fit %>% extract_fit_parsnip()
top10_xgb <- vi(final_xgb_model) %>%
  arrange(desc(Importance)) %>%
  slice_head(n = 10)
cat("\nTop 10 features for XGBoost Model:\n")
print(top10_xgb)
```
It would be interesting to see if there are any features that were commonly identified as important across all three models:
```{r}
# Compute the intersection of the top 10 variable names from each model
common_features <- Reduce(intersect, list(top10_logit$Variable,
                                            top10_rf$Variable,
                                            top10_xgb$Variable))
cat("Common features across all top 10 lists:\n")
print(common_features)

```

We observe that across our Logistic Regression Model with Elastic Net Regularization, our Random Forest Model, and our Boosting Model, these features consistently appeared in the top ten in importance: `hour_cos`, `Heart Rate`, `Respiratory Rate`, `Noninvasive BP Systolic`, `wbc_count`, and `hematocrit`. 

Now, we can chronicle the observed performance on the test set from each of these models, in terms of AUC and Accuracy:
```{r}
logit_metrics <- final_logit_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "Logistic Regression")

rf_metrics <- final_rf_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "Random Forest")

xgb_metrics <- final_xgb_fit %>% 
  collect_metrics() %>% 
  mutate(Model = "XGBoost")

# Combine the metrics and filter for roc_auc and accuracy, using .estimate as the metric value
performance_metrics <- bind_rows(logit_metrics, rf_metrics, xgb_metrics) %>%
  filter(.metric %in% c("roc_auc", "accuracy")) %>%
  select(Model, .metric, .estimate) %>%
  pivot_wider(names_from = .metric, values_from = .estimate)

performance_metrics
```
We observe that the Random Forest and XGBoost models had similar overall performance on the test set, while logistic regression performed slightly worse in terms of these two performance metrics. There is certainly a tradeoff between model accuracy and interpretability to be explored, and the decision to report one model over another should be made with careful consideration of intention and audience. For instance, a statistician prioritizing interpretability such that they can communicate generalizable results to clinician colleagues may opt for logistic regression in this instance, despite its relatively lower performance. Meanwhile, a machine learning engineer hoping to develop a prediction algorithm for length of ICU stay based on clinical features may opt to maximize their performance at the expense of interpretability, while still maintaining generalizable results. An individual participating in a hackathon or statistical modeling competition who simply aims to achieve maximal performance on the unseen test data may fully prioritize performance at the expense of gleaning insights from the model, possibly resorting to more "black-box" complex algorithmic approaches that leverage high computational power. Ultimately, our models all predicted length of ICU stay within a similar "neighborhood" of accuracy, and it will be interesting to compare their individual performance with that of a stacked model.

Now, we create our stacked model. To reduce computational complexity and runtime, we will reduce the number of folds we utilize for each individual model, as well as the size of the grid. 

```{r}
# Instead of 100 penalty levels, let's only do 5
# For mixture, let's do 3 levels
param_grid_logit_small <- grid_regular(
  penalty(range = c(-4, 0)),  # narrower range
  mixture(range = c(0, 1)),   # full range for mixture
  levels = c(5, 3)
)
```

```{r}
set.seed(203)
folds2 <- vfold_cv(train_data, v = 2, strata = los_long)
cl <- makeCluster(parallel::detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})

# Use control_stack_grid() if you plan to use these results in stacking
# DO NOT have a parallel backend running here
# No cluster setup needed


logit_fit2 <- logit_wf %>%
  tune_grid(
    resamples = folds2,
    grid = param_grid_logit_small,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
  )
stopCluster(cl)

```
We can also do our Random Forest with Reduced Folds and grid size:
```{r}
param_grid_rf_small <- grid_regular(
  trees(range = c(100L, 200L)),  # from 100 to 200 trees
  mtry(range = c(1L, 3L)),       # from 1 to 3 for mtry
  levels = c(3, 3)               # 3 levels each → 3 × 3 = 9 combos
)
param_grid_rf_small

set.seed(203)
rf_folds2 <- vfold_cv(train_data, v = 2, strata = los_long)

cl <- makeCluster(parallel::detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})

rf_fit_small <- rf_wf %>%
  tune_grid(
    resamples = rf_folds2,
    grid = param_grid_rf_small,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
  )

stopCluster(cl)

rf_fit_small


```

as well as XGBoost with Reduced Folds and grid size:  
```{r}
param_grid_xgb_small <- grid_regular(
  trees(range = c(100L, 300L)),      # from 100 to 300 trees, 3 levels
  tree_depth(range = c(1L, 3L)),     # from depth 1 to 3, 2 or 3 levels
  learn_rate(range = c(-3, -2), trans = log10_trans()),
  # e.g., 10^-3 to 10^-2 → 2 levels
  levels = c(3, 2, 2)    # 3 × 2 × 2 = 12 total combos
)
param_grid_xgb_small

set.seed(203)
xgb_folds2 <- vfold_cv(train_data, v = 2, strata = los_long)

cl <- makeCluster(parallel::detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(naniar)
  library(lubridate)
  library(glmnet)
  library(vip)
  library(ranger)
})
xgb_fit_small <- xgb_wf %>%
  tune_grid(
    resamples = xgb_folds2,
    grid = param_grid_xgb_small,
    metrics = metric_set(roc_auc, accuracy),
    control = control_stack_grid()
  )

stopCluster(cl)

xgb_fit_small

```

```{r}
# Combine all resample metrics data frames
metrics_combined <- map_dfr(logit_fit2$.metrics, ~ .x)

# Count the distinct .config values
num_configs_filtered <- metrics_combined %>%
  distinct(.config) %>%
  nrow()

cat("Number of candidate configurations in logit_fit_filtered:",
    num_configs_filtered, "\n")
```
Lastly, we will create the stacked model:
```{r}
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)
clusterEvalQ(cl, {
  library(tidyverse)
  library(tidymodels)
  library(GGally)
  library(gtsummary)
  library(glmnet)
  library(vip)
  library(ranger)
  library(stacks)
})
```
Here, we set the penalty term based on autoplot() after fitting the stacked model to our data.

```{r}
# Build the stacking ensemble using the candidate tuning results from your three models.
icu_model_stack <- stacks() %>%
  # Add the candidates from each model. (These objects should have been tuned with control = control_stack_grid().)
  add_candidates(logit_fit2) %>%
  add_candidates(rf_fit_small) %>%
  add_candidates(xgb_fit_small) %>%
  # Blend predictions: this fits a meta-model that combines the candidates.
  blend_predictions(
    penalty = 1e-2,
    metrics = metric_set(roc_auc)
  ) %>%
  # Fit the ensemble members (only those with nonzero stacking coefficients)
  fit_members()

# Display the stacked model object
icu_model_stack

```


```{r}
stopCluster(cl)
```
Now that we have our model, we fit it on the training data and evaluate it on the test set, just as we did for our individual models:
```{r}
# Get probability predictions from the stacked model
stacked_preds <- predict(icu_model_stack, new_data = test_data,
                         type = "prob") %>%
  bind_cols(test_data)

# Convert probabilities into predicted classes (threshold at 0.5)
stacked_preds <- stacked_preds %>%
  mutate(.pred_class = if_else(.pred_TRUE >= 0.5, "TRUE", "FALSE") %>% 
           factor(levels = c("FALSE", "TRUE")))
# Compute ROC AUC
stacked_auc <- stacked_preds %>%
  roc_auc(truth = los_long, .pred_TRUE) %>%
  mutate(Model = "Stacked Ensemble")

# Compute Accuracy
stacked_accuracy <- stacked_preds %>%
  accuracy(truth = los_long, estimate = .pred_class) %>%
  mutate(Model = "Stacked Ensemble")


```
Now we incorporate the stacked model performance into our performance metrics: 
```{r}
stacked_wide <- bind_rows(stacked_auc, stacked_accuracy) %>%
  # pivot them so we get columns "accuracy" and "roc_auc"
  tidyr::pivot_wider(
    names_from = .metric, 
    values_from = .estimate
  )
final_model_performance <- bind_rows(
  performance_metrics, 
  stacked_wide
)

final_model_performance

```

We observe an AUC of 0.3523246 (our lowest by a considerable margin for all models), and an accuracy on par with our other models at 0.6068865. The likely reason for this low AUC is the lack of diversity in our candidate configurations. Due to exorbitant runtimes and issues with R crashes, I opted to limit grid size as well as cross validation. As a result, We miss out on potentially crucial diversity in our model selection process. Additionally, our ensemble model may have discarded more diverse base models based on the training set that would have generalized well to the test set. Lastly, it is possible that our stacked model overfit the data, and it is an unlikely but existent possibility that there are systematic differences between the test and training sets. I intend on diagnosing these issues over time to iterate upon the stacked model and achieve a more satisfactory AUC. Still, the discrepancy in AUC values is undoubtedly insightful and motivating for future ML-focused endeavors. 

Overall, the three individual models performed similarly on training and test data, with XGBoost marginally coming out on top in both metrics. The stacked ensemble model did not seem to add any predictive accuracy to our task, and in fact seems to have compromised ability to perform well at our classification task. Accuracy is just barely larger than that of XGBoost, and AUC is by far the lowest. This is an important consideration for us, considering that AUC is robust to class imbalances. It would be interesting to determine whether model misclassification appears to be random or systematic in some way. I plan to explore these items further in the coming weeks. 

