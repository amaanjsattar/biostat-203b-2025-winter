---
title: "Biostat 203B Homework 1"
subtitle: Due Jan 24, 2025 @ 11:59PM
author: Amaan Jogia-Sattar, 203624648
format:
  html:
    theme: cosmo
    embed-resources: true
    number-sections: false
    toc: true
    toc-depth: 4
    toc-location: left
    code-fold: false
    link-external-icon: true
    link-external-newwindow: true
---

Display machine information for reproducibility:
```{r}
#| eval: true
sessionInfo()
```

## Q1. Git/GitHub

**No handwritten homework reports are accepted for this course.** We work with Git and GitHub. Efficient and abundant use of Git, e.g., frequent and well-documented commits, is an important criterion for grading your homework.

1. Apply for the [Student Developer Pack](https://education.github.com/pack) at GitHub using your UCLA email. You'll get GitHub Pro account for free (unlimited public and private repositories).

**Solution**: I was approved for the Student Developer Pack.

2. Create a **private** repository `biostat-203b-2025-winter` and add `Hua-Zhou` and TA team (`Tomoki-Okuno` for Lec 1; `parsajamshidian` and `BowenZhang2001` for Lec 82) as your collaborators with write permission.

**Solution**: My private GitHub repository has been created, and the specified collaborators have been added with accompanying permissions.

3. Top directories of the repository should be `hw1`, `hw2`, ... Maintain two branches `main` and `develop`. The `develop` branch will be your main playground, the place where you develop solution (code) to homework problems and write up report. The `main` branch will be your presentation area. Submit your homework files (Quarto file `qmd`, `html` file converted by Quarto, all code and extra data sets to reproduce results) in the `main` branch.

**Solution**: I have created a `hw1` directory and am actively maintaining two branches: `main` and `develop`. While working through homework assignments, I operate in the `develop` branch, and I will merge the changes with the `main` branch upon successful completion. The `qmd` and rendered `html` files are present in my `hw1` directory, and any necessary additional files will be similarly present in `main`.

4. After each homework due date, course reader and instructor will check out your `main` branch for grading. Tag each of your homework submissions with tag names `hw1`, `hw2`, ... Tagging time will be used as your submission time. That means if you tag your `hw1` submission after deadline, penalty points will be deducted for late submission.

**Solution**: I will tag my homework submissions with the provided tag names to timestamp my assignments.

5. After this course, you can make this repository public and use it to demonstrate your skill sets on job market.

**Solution**: I will publicize this repository at the close of this course to demonstrate my acquired skill set.

## Q2. Data ethics training

This exercise (and later in this course) uses the [MIMIC-IV data v3.1](https://physionet.org/content/mimiciv/3.1/), a freely accessible critical care database developed by the MIT Lab for Computational Physiology. Follow the instructions at <https://mimic.mit.edu/docs/gettingstarted/> to (1) complete the CITI `Data or Specimens Only Research` course and (2) obtain the PhysioNet credential for using the MIMIC-IV data. Display the verification links to your completion report and completion certificate here. **You must complete Q2 before working on the remaining questions.** (Hint: The CITI training takes a few hours and the PhysioNet credentialing takes a couple days; do not leave it to the last minute.)

**Solution**: [Citi Completion Report](https://www.citiprogram.org/verify/?k9f2ad8d9-7bc6-4a7e-9a3a-267a3e5581cf-67219680)
[CITI Completion Certificate](https://www.citiprogram.org/verify/?w81e29d37-3ca5-4d3e-8e1d-04daf9f458c3-67219680)

## Q3. Linux Shell Commands

1. Make the MIMIC-IV v3.1 data available at location `~/mimic`. The output of the `ls -l ~/mimic` command should be similar to the below (from my laptop).
```{bash}
#| eval: true
# content of mimic folder
ls -l ~/mimic/
```
Refer to the documentation <https://physionet.org/content/mimiciv/3.1/> for details of data files. Do **not** put these data files into Git; they are big. Do **not** copy them into your directory. Do **not** decompress the gz data files. These create unnecessary big files and are not big-data-friendly practices. Read from the data folder `~/mimic` directly in following exercises. 

**Solution**: I have downloaded the MIMIC-IV v3.1 data, and it is available at the specified location `~/mimic`. I have examined the file structure and adhered to all guidelines mentioned. 

  Use Bash commands to answer following questions.

2. Display the contents in the folders `hosp` and `icu` using Bash command `ls -l`. Why are these data files distributed as `.csv.gz` files instead of `.csv` (comma separated values) files? Read the page <https://mimic.mit.edu/docs/iv/> to understand what's in each folder.

**Solution**: Here is the content of the `hosp` folder: 
```{bash}
ls -l ~/mimic/hosp/

```
Here is the content of the `icu` folder: 
```{bash} 
ls -l ~/mimic/icu/
```
We observe that the data files are compressed, existing in `csv.gz` rather than `.csv` file format. The `.gz` in the naming convention refers to the method of compression, in this case being the `gzip` algorithm. This form of compression is standard practice when working with Linux and Unix operating systems. This compressed file format significantly reduces the file size, which is particularly important when handling 'big data'. This aids in both transferability and storage efficiency. Keeping the files in compressed format saves considerable storage space and bandwidth. Files can be read directly within most settings (e.g. R, Python, Linux) without the need for immediate manual decompression. My local download is over 10.66 GB large. In order to reduce the computational tolls associated with such large data, compression is a vastly useful implement.

3. Briefly describe what Bash commands `zcat`, `zless`, `zmore`, and `zgrep` do.

**Solution**: 

`zcat`: This command outputs the uncompressed content of a `.gz` compressed file to a user's terminal without directly decompressing the file itself. This allows the user to view the contents of the file without directly modifying it through decompression, which can produce intermediate files that may not be necessary for the user's task at hand.

`zless`: This command outputs the uncompressed content of a `.gz` compressed file to a user's terminal in a paginated fashion. This allows the user to interact with and peruse the file contents at their leisure. This command is often useful in the context of manual file exploration.

`zmore`: This command is similar to `zless` in that it outputs the uncompressed content of a `.gz` compressed file to a user's terminal in a paginated fashion. However, this command utilizes the `less` command for pagination. In fact, both commands have the analogues `more` and `less`, which are applicable to non-compressed files. Subtle differences exist in the navigational features of each command. While `zless` allows for forward and reverse scrolling, `zmore` only allows for forward scrolling. `zless` offers search options and does not need to read an entire input file before starting. Ultimately, it is common convention to utilize `zless` and `less` in place of `zmore` and `more` for common purposes. 

`zgrep`: This command is utilized to search for text patterns within compressed `.gz` files by applying the `grep` command to a `.gz` compressed file. This allows users to search for patterns within a file without the need for explicit decompression. Much like the aforementioned commands, this allows for straightforward filtering and pattern recognition within large, compressed files without decompressing them beforehand.



4. (Looping in Bash) What's the output of the following bash script?
```{bash}
#| eval: true
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  ls -l $datafile
done
```
**Solution**: 

This bash script contains a `for loop`, which iterates through all the files in the `hosp` subfolder of our `mimic` dataset that match our specified criteria. We are ultimately returning a list of files in `long list` format, via the `ls -l` command. This returns one line per file, including the following information: file permissions, owner and group, file size (bytes), date of last modification, and filename. In this case, we specify patterns to identify in the filenames. The bracketed segment is referred to as an `expansion`, allowing us to specify different starting strings for different patters. The asterisk is a `wildcard` character, matching zero or more characters in our filenames. Lastly, we specify that our filename should conclude with a `.gz`. Essentially, the script will return the listed names of files in the hospital subfolder that end with `.gz` and start with either `a`,`l`, or `pa`. We can see that this will return the `admissions`, `labevents`, and  `patients` files.

Display the number of lines in each data file using a similar loop. (Hint: combine linux commands `zcat <` and `wc -l`.)

**Solution**: 
```{bash}
#| eval: false
for datafile in ~/mimic/hosp/{a,l,pa}*.gz
do
  echo "Processing file: $datafile"
  zcat "$datafile" | wc -l
done

```

5. Display the first few lines of `admissions.csv.gz`. How many rows are in this data file, excluding the header line? Each `hadm_id` identifies a hospitalization. How many hospitalizations are in this data file? How many unique patients (identified by `subject_id`) are in this data file? Do they match the number of patients listed in the `patients.csv.gz` file? (Hint: combine Linux commands `zcat <`, `head`/`tail`, `awk`, `sort`, `uniq`, `wc`, and so on.)

**Solution**: Display the first few lines of `admissions.csv.gz`.
```{bash}
zless < ~/mimic/hosp/admissions.csv.gz | head -n 5

```
**Solution**: How many rows in `admissions.csv.gz`, excluding header?
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
wc -l
```
**Solution**: How many hospitalizations in `hadm_id`?
```{bash} 
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
awk -F',' '{print $2}' | 
sort | 
uniq | 
wc -l

```
**Solution**: How many unique patients (identified by `subject_id`) are in this data file?
```{bash}

zcat < ~/mimic/hosp/admissions.csv.gz |
tail -n +2 | 
awk -F',' '{print $1}' | 
sort | 
uniq | 
wc -l
```
**Solution**: Compare the number of unique patients in `admissions.csv.gz` to the number of unique patients in `patients.csv.gz`. 

First, we glance at the first few lines of the file.
```{bash} 
zless < ~/mimic/hosp/patients.csv.gz | head -n 5
```
Next, we count the number of unique values present in the `subject_id` column (the first column):
```{bash}
zcat < ~/mimic/hosp/patients.csv.gz |
tail -n +2 | 
awk -F',' '{print $1}' | 
sort | 
uniq | 
wc -l
```
We observe that there are 223,452 patients in the `admissions.csv.gz`, and 364,627 patients in the `patients.csv.gz` file. These numbers clearly do not match.

6. What are the possible values taken by each of the variable `admission_type`, `admission_location`, `insurance`, and `ethnicity`? Also report the count for each unique value of these variables in decreasing order. (Hint: combine Linux commands `zcat`, `head`/`tail`, `awk`, `uniq -c`, `wc`, `sort`, and so on; skip the header line.)

**Solution**:  `admission_type`

We observe that the possible values for `admission_type` are: `EW EMER.`, `EU OBSERVATION`, `OBSERVATION ADMIT`, `URGENT`, `SURGICAL SAME DAY ADMISSION`, `DIRECT OBSERVATION`, `DIRECT EMER.`, `ELECTIVE`, and `AMBULATORY OBSERVATION`. Frequencies for each unique value are displayed below, in descending order:
```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
awk -F',' '{print $6}' | 
sort | 
uniq -c | 
sort -nr 
```
**Solution**: `admission_location`

We observe that the possible values for `admission_location` are: `EMERGENCY ROOM`, `PHYSICIAN REFERRAL`, `TRANSFER FROM HOSPITAL`, `WALK-IN/SELF REFERRAL`, `CLINIC REFERRAL`, `PROCEDURE SITE`, `TRANSFER FROM SKILLED NURSING FACILITY`, `INTERNAL TRANSFER TO OR FROM PSYCH`, `PACU`, `INFORMATION NOT AVAILABLE`, `AMBULATORY SURGERY TRANSFER`, and ` ` (a singular blank entry). Frequencies for each unique value are displayed below, in descending order:

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
awk -F',' '{print $8}' | 
sort | 
uniq -c | 
sort -nr
```
**Solution**: `insurance`

We observe that the possible values for `insurance` are: `Medicare`, `Private`, `Medicaid`, `Other`, ` ` (blank entry), and `No charge`. Frequences for each unique value are displayed below, in descending order:

```{bash}
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
awk -F',' '{print $10}' | 
sort | 
uniq -c | 
sort -nr
```
**Solution**: `race`

We observe that the possible values for `race` are `WHITE - RUSSIAN`, `WHITE - OTHER EUROPEAN`, `WHITE - EASTERN EUROPEAN`, `WHITE - BRAZILIAN`, `WHITE`, `UNKNOWN`, `UNABLE TO OBTAIN`, `SOUTH AMERICAN`, `PORTUGUESE`, `PATIENT DECLINED TO ANSWER`, `OTHER`, `NATIVE HAWAIIAN OR OTHER PACIFIC ISLANDER`, `MULTIPLE RACE/ETHNICITY`, `HISPANIC/LATINO - SALVADORAN`, `HISPANIC/LATINO - PUERTO RICAN`, `HISPANIC/LATINO - MEXICAN`, `HISPANIC/LATINO - HONDURAN`, `HISPANIC/LATINO - GUATEMALAN`, `HISPANIC/LATINO - DOMINICAN`, `HISPANIC/LATINO - CUBAN`, `HISPANIC/LATINO - COLUMBIAN`, `HISPANIC/LATINO - CENTRAL AMERICAN`, `HISPANIC OR LATINO`, `BLACK/CARIBBEAN ISLAND`, `BLACK/CAPE VERDEAN`, `BLACK/AFRICAN AMERICAN`, `BLACK/AFRICAN`, `ASIAN - SOUTH EAST ASIAN`, `ASIAN - KOREAN`, `ASIAN - CHINESE`, `ASIAN - ASIAN INDIAN`, `ASIAN`, and `AMERICAN INDIAN/ALASKA NATIVE`. Frequencies for each unique value are displayed below, in descending order. 

```{bash} 
zcat < ~/mimic/hosp/admissions.csv.gz | 
tail -n +2 | 
awk -F',' '{print $13}' | 
sort | 
uniq | 
sort -nr
```

7. The `icustays.csv.gz` file contains all the ICU stays during the study period. How many ICU stays, identified by `stay_id`, are in this data file? How many unique patients, identified by `subject_id`, are in this data file?

**Solution**: Total ICU Stays in `icustays.csv.gz`

First, we will examine the first few rows of the file, including the header to determine the column numbers for `stay_id` and `subject_id`:

```{bash}
zless < ~/mimic/icu/icustays.csv.gz | head -n 5
```
Next, we will count the total entries in the `stay_id` column to determine the total number of ICU stays:
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | 
tail -n +2 | 
awk -F',' '{print $3}' | 
wc -l
```
Next, we will count the total **unique** entries in the `subject_id` column to determine the total number of unique patients:
```{bash}
zcat < ~/mimic/icu/icustays.csv.gz | 
tail -n +2 | 
awk -F',' '{print $1}' | 
sort | 
uniq |  
wc -l
```
We observe that there are 94,458 unique ICU stays in the file and 65,366 unique patients in the file.

8. _To compress, or not to compress. That's the question._ Let's focus on the big data file `labevents.csv.gz`. Compare compressed gz file size to the uncompressed file size. Compare the run times of `zcat < ~/mimic/labevents.csv.gz | wc -l` versus `wc -l labevents.csv`. Discuss the trade off between storage and speed for big data files. (Hint: `gzip -dk < FILENAME.gz > ./FILENAME`. Remember to delete the large `labevents.csv` file after the exercise.)

**Solution**: Compressed vs. Uncompressed

First, we will compare the file sizes for the compressed and uncompressed versions of `labevents.csv.gz`. To check the file size for the compressed file, we run the following: 

```{bash} 
ls -lh ~/mimic/hosp/labevents.csv.gz

```
To check the file size for the uncompressed file, we must decompress the file and then list details. We will use the `-dk` option to specify that we wish to keep the original file instead of deleting it. We run the following:
```{bash}
#| eval: true
gzip -dk < ~/mimic/hosp/labevents.csv.gz > ~/mimic/hosp/labevents.csv
```

Now we will check the file size: 
```{bash}
#| eval: true
ls -lh ~/mimic/hosp/labevents.csv
```
We observe that the compressed file size is 2.4 Gb, while the uncompressed file size is 17 Gb. This is a monumental difference in required storage. We will proceed to check the runtimes for counting lines of the compressed versus uncompressed file. Beginning with the compressed file, we run the following: 
```{bash}
time zcat < ~/mimic/hosp/labevents.csv.gz | wc -l
```
Now, we wil check the runtime for counting the lines of our uncompressed csv file. We run the following: 
```{bash} 
#| eval: true
time wc -l ~/mimic/hosp/labevents.csv
```
We observe that counting the lines in the compressed file took approximately 1.5 minutes (1 minute and 29.706 seconds at the time this was written, to be exact). Meanwhile, counting the lines in the uncompressed file took approximately 23 seconds (22.678 seconds at the time this was written, to be exact). This means that this same operation performed on the uncompressed file was nearly four times faster, which can be attributed to the fact that we do not need to complete the intermediary step of running `zcat` to display the uncompressed file contents before word counting. CLearly, there is a tradeoff between storage and speed when working with large data files. Uncompressing large files can be costly in terms of storage, but it allows for faster runtimes on processes involving said files. Conversely, working with compressed files can be less costly from a storage perspective, but operations pertaining to the uncompressed contents of the file will rely on the aforementioned intermediary step, increasing runtime. The choice of whether or not to compress files will vary based on the user's resource availability, patience, and value judgement. For users with limited storage capacity (i.e. users who download all data locally), it is likely that the slower runtime may be preferable to downloading extremely large files locally without the storage capacity for them. For users who may need to perform more sophisticated operations on unzipped files, it may be a worthwhile tradeoff to uncompress files and operate on them directly at the expense of some storage space. Similarly, if a file is only to be used once in a particular pipeline, it may be preferable to work with a compressed version as it is a 'one-time' lag in runtime. However, if the file is of central importance in the overarching endeavor, utilizing an uncompressed version may greatly improve the total runtime at multiple points throughout the pipeline. Ultimately, the decision to compress a file should be made in thorough consideration of its utility in fulfilling user goals and the user's own storage reserves. 

Now that we have concluded this exercise, we will delete the uncompressed file. We run the following: 
```{bash}
#| eval: true
rm ~/mimic/hosp/labevents.csv
```

## Q4. Who's popular in Price and Prejudice

1. You and your friend just have finished reading *Pride and Prejudice* by Jane Austen. Among the four main characters in the book, Elizabeth, Jane, Lydia, and Darcy, your friend thinks that Darcy was the most mentioned. You, however, are certain it was Elizabeth. Obtain the full text of the novel from <http://www.gutenberg.org/cache/epub/42671/pg42671.txt> and save to your local folder. 
```{bash}
#| eval: true
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
```
Explain what `wget -nc` does. Do **not** put this text file `pg42671.txt` in Git. Complete the following loop to tabulate the number of times each of the four characters is mentioned using Linux commands.
```{bash}
#| eval: true
wget -nc http://www.gutenberg.org/cache/epub/42671/pg42671.txt
for char in Elizabeth Jane Lydia Darcy
do
  echo $char:
  grep -ow "$char" pg42671.txt | wc -l
done
```

**Solution** Pride and Prejudice

We utilize `wget` to download files from the web via our bash chunk or command line. This allows us to non-interactively download a web-based file locally. The `-nc` specification stands for `no clobber` and it prevents any redownloading of the file if it already exists in our directory. Hence, if the file is identified to be present in the directory already, this specification ensures that the file is not retrieved an additional time. 
We complete the loop above, which tabulates the number of times each of the four characters is mentioned using our Linux commands. It appears that our friend was incorrect about Darcy being the most mentioned character in the book. We were correct in our assertion that Elizabeth is the character mentioned by name most, at 634 times compared to Darcy's 416 times. 

2. What's the difference between the following two commands?
```{bash}
#| eval: true
echo 'hello, world' > test1.txt
```
and
```{bash}
#| eval: true
echo 'hello, world' >> test2.txt
```
**Solution** Difference Between Commands

The first command writes `hello, world` to a file called `test1.txt`. It creates the file if it does not already exist. If the file does exist, this command overwrites its contents, replacing anything prior with `hello, world`. In contrast, the second command appends `hello, world` to a file called `test2.txt`. If the file does not already exist, it creates the file and writes `hello, world` as its contents. If the file already exists and is populated with written content, then it adds `hello, world` at the end of the file and preserves the preexisting contents. This subtle difference can be illustrated by running both cells above multiple times. Notice that after running the bash commands several times and opening each file, `test1.txt` will still be a one-line file with its only contents being the specified `hello, world` message. Meanwhile, `test2.txt` will have a `hello, world` line for each time the bash code was run. Ultimately, this illustrates the difference between overwriting preexisting content and appending it with new content. 

3. Using your favorite text editor (e.g., `vi`), type the following and save the file as `middle.sh`:
```{bash eval=FALSE}
#!/bin/sh
# Select lines from the middle of a file.
# Usage: bash middle.sh filename end_line num_lines
head -n "$2" "$1" | tail -n "$3"
```
Using `chmod` to make the file executable by the owner, and run
```{bash}
#| eval: true
./middle.sh pg42671.txt 20 5
```
Explain the output. Explain the meaning of `"$1"`, `"$2"`, and `"$3"` in this shell script. Why do we need the first line of the shell script?

**Solution** middle.sh

We see that the output of running our script with the provided parameter values is the release date and language of the book. The script took `pg42671.txt` as its input, reads the first 20 lines of the file, then keeps the last 5 lines from this set. The `"$1"`, `"$2"`, and `"$3"` in our shell script serve as `positional parameters` that are passed to the script when it is run. In our case, these correspond to the arguments we provided above. Namely, `"$1"` pertains to the name of the file we aim to process (in this case, `pg42671.txt`), `"$2"` pertains to the number of lines we extract from the file's beginning in our initial output via `head`. In this case, we select the first 20 lines of the file. Lastly, `"$3"` pertains to the number of lines from our 20-line selection we ultimately extract, starting at the end of the file. In our case, we extract the last 5 lines from the end of our 20-line selection. In summary, the process is as follows: process `pg42671.txt` as our chosen file -> get the first 20 lines from the file -> output the last five lines of this 20-line selection. The first line of the shell script reads `#!/bin/sh`. This character sequence is known as a `shebang` or `hashbang`, and it tells a Unix-like operating system which interpreter to use when executing our script. When running the script, the operating system looks for the shebang line and utilizes the specified interpreter or command to execute our script. This ultimately ensures correct interpretation of our bash script, such that it will do what we want it to do. 

## Q5. More fun with Linux

Try following commands in Bash and interpret the results: `cal`, `cal 2025`, `cal 9 1752` (anything unusual?), `date`, `hostname`, `arch`, `uname -a`, `uptime`, `who am i`, `who`, `w`, `id`, `last | head`, `echo {con,pre}{sent,fer}{s,ed}`, `time sleep 5`, `history | tail`.

**Solution**: Interpreting Commands
```{bash}
cal
```
```{bash}
cal 2025
```

```{bash}
cal 9 1752
```
```{bash}
date
```
```{bash}
hostname
```

```{bash}
arch
```
```{bash}
uname -a
```
```{bash}
uptime
```

```{bash}
who am i
```

```{bash}
who

```

``` {bash}
w
```

``` {bash}
id
```

``` {bash}
last | head
```
``` {bash}
echo {con,pre}{sent,fer}{s,ed}
```

```{bash}
time sleep 5
```

```{bash}
history | tail
```

`cal`: Displays the calendar for the current month.  
`cal 2025`: Displays the calendar for the year 2025.  
`cal 9 1752`: Shows the calendar for September 1752, missing 11 days due to the Gregorian calendar adoption.  
`date`: Displays the current date and time.  
`hostname`: Shows the name of the machine (host).  
`arch`: Displays the architecture of the system.  
`uname -a`: Outputs detailed system information, including kernel, OS, and architecture.  
`uptime`: Shows how long the system has been running and the load averages.  
`who am i`: Displays the current userâ€™s login session information.  
`who`: Lists all users currently logged into the system.  
`w`: Displays detailed information about logged-in users and their processes.  
`id`: Outputs the user ID (`UID`), group ID (`GID`), and group memberships of the current user.  
`last | head`: Displays the most recent login sessions.  
`echo {con,pre}{sent,fer}{s,ed}`: Uses brace expansion to generate combinations of words.  
`time sleep 5`: Measures the time taken to execute a command (e.g., `sleep 5`).  
`history | tail`: Displays the last few commands executed in the shell history.  

## Q6. Book
1. Git clone the repository <https://github.com/christophergandrud/Rep-Res-Book> for the book _Reproducible Research with R and RStudio_ to your local machine. Do **not** put this repository within your homework repository `biostat-203b-2025-winter`.

2. Open the project by clicking `rep-res-3rd-edition.Rproj` and compile the book by clicking `Build Book` in the `Build` panel of RStudio. (Hint: I was able to build `git_book` and `epub_book` directly. For `pdf_book`, I needed to add a line `\usepackage{hyperref}` to the file `Rep-Res-Book/rep-res-3rd-edition/latex/preabmle.tex`.)

The point of this exercise is (1) to obtain the book for free and (2) to see an example how a complicated project such as a book can be organized in a reproducible way. Use `sudo apt install PKGNAME` to install required Ubuntu packages and `tlmgr install PKGNAME` to install missing TexLive packages.

For grading purpose, include a screenshot of Section 4.1.5 of the book here.
**Solution**: Book Download

I was able to successfully download and compile the book locally. A screenshot of the specified section is included in the `hw1` folder, as well as below: 
![Section 4.1.5 Screenshot](RepRes_Screenshot.png)
---
